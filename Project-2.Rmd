---
title: "Project 2"
author: "Joey Chen and John Williams"
date: "10/24/2021"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

# necessary packages
library(tidyverse)
library(corrplot)
library(caret)
library(modelr)
```

# Introduction

This project is a simple walk-through of these steps of the data science process:


>Data Preparation --> Exploratory Data Analysis --> Model Selection --> Model Evaluation


We'll be using the `OnlineNewsPopularity` dataset from the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/). It includes continuous and categorical variables about articles published by Mashable (www.mashable.com) over a period of two years. Our goal is to predict the popularity of articles in social networks (number of `shares`) using 58 predictive variables.

The 58 predictive variables can be grouped together by the similar attributes each measures. There are integer variables that measure the number of words, keywords, hyperlinks, images, and videos within the article itself and its title; binary variables that categorize on what day of the week the article was published; variables that measure the rate of positive/negative words in the content; and variables measuring polarity, subjectivity, and sentiment of the article or title. For more information on the dataset and detailed descriptions of the predictive variables, click [here](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity).

After completing EDA, we will describe and demonstrate building 4 predictive models: 2 linear regression models and 2 ensemble models (Random Forest and Boosted Tree). To build and evaluate these models, we will split the data into a "training" set and "testing" set (70% and 30%, respectively). Finally, we will select the model that has the lowest root mean square error (RMSE) as the "best fit".

# Data Preparation

```{r data}
# read data
news_data <- read_csv("./OnlineNewsPopularity.csv")

# data cleaning
world_news_data <- news_data %>% 
                   # automation to subset data by data channel
                   filter(eval(parse(text = paste0("data_channel_is_", 
                                                   params[[1]]))) == 1) %>%
                   # remove data channel variables
                   select(-starts_with("data_channel_is")) %>%
                   # remove non-predictive variables
                   select(-url, -timedelta)
```

# Exploratory Data Analysis

## Distribution of Response Variable

First, we can take a look at the distribution of our response variable `shares`. We can first look at the histogram. 

```{r Joey EDA shares}
# Histogram of shares
ggplot(world_news_data, aes(x=shares)) +
  geom_histogram(bins=50, fill="navy", col="darkgreen") +
  labs(title="Histogram of shares")
```

From the histogram We can see that the distribution is heavily right skewed. The histogram scale may be influenced by potential outliers, so we can look at the distribution of `shares` under 5,000. 

```{r Joey EDA shares le 5000}
# Histogram of shares < 5000
ggplot(filter(world_news_data, shares < 5000), aes(x=shares)) +
  geom_histogram(bins=50, fill="navy", col="darkgreen") +
  labs(title="Histogram of shares under 5000")
```
  
As we can see, the distribution is still heavily right skewed. So we may want to look at the distribution of `log(shares)`.
  
```{r Joey EDA log shares}
# Histogram of log(shares) 
ggplot(world_news_data, aes(x=log(shares))) +
  geom_histogram(bins=50, fill="navy", col="darkgreen") +
  labs(title="Histogram of log(shares)")
```

From the histogram we can see that the distribution is slightly right skewed. But it is much closer to normal compared to the original distribution. 
  
  
We can also look at the numeric summary of `shares` vs `log(shares)`.

```{r Joey EDA log shares summary}
# Numeric summary of shares
shares_summary <- world_news_data %>% summarise(Min. = min(shares),
                                                Q1 = quantile(shares, 0.25),
                                                Median = median(shares),
                                                Mean = mean(shares),
                                                SD = sd(shares),
                                                Q3 = quantile(shares, 0.75),
                                                Max = max(shares),
                                                CV = scales::percent(SD / Mean))

knitr::kable(shares_summary, digits=0, caption = "Numeric Summary of Shares")

# Numeric summary of log(shares)
log_shares_summary <- world_news_data %>% summarise(Min. = min(log(shares)),
                                                Q1 = quantile(log(shares), 0.25),
                                                Median = median(log(shares)),
                                                Mean = mean(log(shares)),
                                                SD = sd(log(shares)),
                                                Q3 = quantile(log(shares), 0.75),
                                                Max = max(log(shares)),
                                                CV = scales::percent(SD / Mean))

knitr::kable(log_shares_summary, digits=3, caption="Numeric Summary of log(Shares)")
```
In addition to the histograms, we can see that the coefficient of variation (CV) is much larger in `shares` compared to `log(shares)`. This can make `shares` harder to predict since the variability is so high. So we will continue to do EDA using `log(shares)` and will fit the models on `log(shares)` since that can also help reduce the impact of some extreme values of `shares`.  

## Log(shares) by Number of Words in the Title

We can investigate how the number of words in the title might impact log(shares). We can start by looking at the distribution of the title word count. We can see which title word counts are used the most within this data. 

```{r Joey EDA Title Words bar}
ggplot(data=world_news_data, aes(x=n_tokens_title)) +
  geom_bar(col="darkblue", fill="darkgreen") +
  labs(title="Bar graph of Number of Words in Title",
       x="Number of Words in the Title")
```

Here is the numerical summary. We can see which title word count has the highest mean or median log(shares). 

```{r Joey EDA Title Words summary}
words_title_summary <- world_news_data %>% group_by("Title Word Count" = n_tokens_title) %>% summarise(n=length(log(shares)),
                                                Min. = min(log(shares)),
                                                Q1 = quantile(log(shares), 0.25),
                                                Median = median(log(shares)),
                                                Mean = mean(log(shares)),
                                                SD = sd(log(shares)),
                                                Q3 = quantile(log(shares), 0.75),
                                                Max = max(log(shares)))

knitr::kable(words_title_summary, digit=2, caption="Summary Log(shares) by Number of Words in the Title")
```

We can also assess the relationship between log(shares) and title word count by looking at the boxplot. The red dot represents the mean. Higher red dot means the articles are shared more on average, for that number of words in title. However, the mean can be heavily influenced by outliers.  

```{r Joey EDA Title Words Boxplot}
ggplot(world_news_data, aes(x = as.factor(n_tokens_title), y = log(shares))) +
  geom_boxplot(aes(fill=as.factor(n_tokens_title))) +
  stat_summary(fun.y="mean", col="red") +
  theme(legend.position = "none") +
  labs(title="Boxplot of Log(shares) by Number of Words in title", 
       x = "Number of Words in Title")
```

## Log(shares) by Day of Week

Let's create a categorical variable `day_published` and determine if the mean/median number of shares changes depending on what day the article was published: 

```{r John EDA1}
world_news_data2 <- world_news_data %>%
                    pivot_longer(starts_with("weekday"), 
                                 names_to = "day_published",
                                 names_prefix = "weekday_is_") %>%
                    filter(value == 1) %>%
                    select(-value)

world_news_data2$day_published <- factor(world_news_data2$day_published,
                                         levels=c("monday", "tuesday", 
                                                  "wednesday", "thursday", 
                                                  "friday", "saturday", 
                                                  "sunday"))

ggplot(world_news_data2, aes(x = day_published, y = log(shares))) +
  geom_boxplot(aes(fill = day_published)) +
  stat_summary(fun = "mean") +
  theme(legend.position = "none") +
  labs(x = "Day of Publication", 
       title = "Log(Shares) by Day of Publication")
```

After viewing the boxplot above, you should be able to determine what day (or days) have the highest mean/median shares compared to other days.  The median is represented by the bold horizontal black line ithin the colored box; the mean is represented by the black dot within the colored box. 

We can further see evidence of this by examining the categorical variable `is_weekend`:

```{r John EDA2}
ggplot(world_news_data, aes(x = as.factor(is_weekend), y = log(shares))) +
  geom_boxplot(aes(fill = as.factor(is_weekend))) +
  theme(legend.position = "none") +
  labs(x = "Published on Weekend?  0 = No, 1 = Yes", 
       title = "Log(Shares) by Day of Publication")
```

## Summary by Interval

Let's examine the relationship between a continuous variable that is within the range [0, 1] and `log(shares)`. One way we can do this by "cutting" the variable into 11 subintervals ((-0.5, 0.5], (0.5, 1.5], (1.5, 2.5], etc.) and calculating the mean/median `log(shares)` for each subinterval. If the mean/median of `log(shares)` steadily increases as the predictor increases, then there is a positive relationship; if the mean/median of `log(shares)` steadily decreases as the predictor increases, then there is a negative relationship. If there is no clear pattern in the mean/median of `log(shares)` as the predictor increases, then we cannot make any statement about the linear relationship of that predictor and the response. For example, `title_subjectivity` has a range [0, 1]. Let's see how the mean/median of `log(shares)` changes as `title_subjectivity` increases:

```{r John EDA3}
tab <- world_news_data %>%
       mutate(title_subjectivity = cut(title_subjectivity, 
                                       seq(-0.05, 1.05, by = 0.1))) %>%
       group_by(title_subjectivity) %>%
       summarise(mean = mean(log(shares)), 
                 median = median(log(shares)), 
                 n = n())

knitr::kable(tab, 
             digits = 3,
             format.args = list(big.mark = ",", scientific = FALSE),
             col.names = c('Title Subjectivity', 'Mean', 'Median', 'Count'),
             caption = "Summary of Title Subjectivity")
```

Similarly, we can examine `rate_negative_words`:

```{r JOhn EDA4}
tab <- world_news_data %>%
       mutate(rate_negative_words = cut(rate_negative_words, 
                                        seq(-0.05, 1.05, by = 0.1))) %>%
       group_by(rate_negative_words) %>%
       summarise(mean = mean(log(shares)), 
                 median = median(log(shares)), 
                 n = n())

knitr::kable(tab, 
             digits = 3,
             format.args = list(big.mark = ",", scientific = FALSE),
             col.names = c('Rate Negative Words', 'Mean', 'Median', 'Count'),
             caption = "Summary of Rate Negative Words")
```

And `avg_positive_polarity`:

```{r John EDA5}
tab <- world_news_data %>%
       mutate(avg_positive_polarity = cut(avg_positive_polarity, 
                                          seq(0, 1, by = 0.1), right = F)) %>%
       group_by(avg_positive_polarity) %>%
       summarise(mean = mean(log(shares)), 
                 median = median(log(shares)), 
                 n = n())

knitr::kable(tab, 
             digits = 3,
             format.args = list(big.mark = ",", scientific = FALSE),
             col.names = c('Avg Positive Polarity', 'Mean', 'Median', 'Count'),
             caption = "Summary of Average Positive Polarity")
```

## Log(shares) by Number of Keywords

Is the number of keywords influential in determining the number of shares? Let's transform `num_keywords` into a categorical variable and create a boxplot. The boxplot will show us if any particular number of keywords is related to higher number of shares.

```{r John EDA6}
ggplot(world_news_data, aes(x = as.factor(num_keywords), y = log(shares))) +
  geom_boxplot(aes(fill=as.factor(num_keywords))) +
  stat_summary(fun.y = "mean") +
  theme(legend.position = "none") +
  labs(x = "Number of Keywords", 
       title = "Log(Shares) by Number of Keywords")
```
  
## Log(shares) by Number of Images and Videos

We can examine the impact of the number of videos and images on `log(shares)`. We can do this by looking at the scatterplots. If the points show an upward trend, then articles with more images or videos would be shared more often. If we see a negative trend then articles with more images or videos would be shared less often. 

```{r Joey EDA img video}
# Scatterplot of num_img and log(shares)
ggplot(world_news_data, aes(x = num_imgs, y = log(shares))) +
  geom_point() + 
  geom_smooth(method="lm") +
  labs(title="Scatterplot of Number of Images vs log(shares)",
       x="Number of Images")

# Scatterplot of num_videos and log(shares)
ggplot(world_news_data, aes(x = num_videos, y = log(shares))) +
  geom_point() + 
  geom_smooth(method="lm") +
  labs(title="Scatterplot of Number of Videos vs log(shares)",
       x="Number of Videos")
```

We can also examine the correlation coefficient (r) to observe the strength of the association. For absolute values of r, we can considered 0-0.19 as very weak, 0.2-0.39 as weak, 0.4-0.59 as moderate, 0.6-0.79 as strong and 0.8-1 as very strong correlation. 

```{r Joey EDA img video corr}
# Correlation Coefficient (r) of num_imgs vs log(shares) 
cor(x = world_news_data$num_imgs, y = log(world_news_data$shares))

# Correlation Coefficient (r) of num_videos vs log(shares) 
cor(x = world_news_data$num_videos, y = log(world_news_data$shares))
```


## Correlation of Predictors

Now, before creating our prediction models, we need to examine the correlation among predictors, as we would not want to include predictors in the model that are highly correlated. Since we have a large number of predictors, let's attempt to subset the predictors based on measuring similar attributes.

```{r John EDA7}
# Create a correlogram for predictors that measure attributes of "tokens" 
# and "words"
cor1 <- world_news_data %>% select(contains("tokens"), contains("words"))
cormatrix <- round(cor(cor1), 2)
corrplot(cormatrix, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)

# Create a correlogram for predictors that measure attributes of keywords 
cor2 <- world_news_data %>% select(contains("keywords"), contains("kw"))
cormatrix <- round(cor(cor2), 2)
corrplot(cormatrix, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)

# Create a correlogram for predictors that measure attributes of "positve" and
# "negative" connotation 
cor3 <- world_news_data %>% select(contains("positive"), contains("negative"))
cormatrix <- round(cor(cor3), 2)
corrplot(cormatrix, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)

# Create a correlogram for predictors that measure attributes of article titles
cor4 <- world_news_data %>% select(contains("title"))
cormatrix <- round(cor(cor4), 2)
corrplot(cormatrix, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```

# Model Selection

```{r Partitioning of Train and Test Data}
set.seed(31415)

partition <- createDataPartition(y = world_news_data$shares, 
                                 p= 0.7, 
                                 list = FALSE)
world_news_train <- world_news_data[partition,]
world_news_test <- world_news_data[-partition,]
```

Linear regression models a response variable as a linear combination of input predictor variables. The models are often fitted using the least squares approach, which is a method that finds the best fit by minimizing the sum of the squares of the residuals of the points from the curve. When we fit the model on the training data we would get coefficients corresponding to each of the predictor variables, and these coefficients can be used to make predictions on the response with predictor variables from the test data.
  
## Linear Model #1

```{r Joey Linear Model 1}
lm1Fit <- lm(log(shares) ~ n_non_stop_words + 
                     num_hrefs +
                     num_imgs + 
                     num_videos +
                     average_token_length + 
                     kw_min_min +
                     kw_min_avg +
                     kw_max_avg +
                     kw_avg_avg +
                     self_reference_avg_sharess +
                     weekday_is_monday +
                     weekday_is_tuesday +
                     weekday_is_wednesday +
                     weekday_is_thursday +
                     weekday_is_friday +
                     LDA_00 +
                     LDA_02 +
                     global_subjectivity +
                     title_sentiment_polarity +
                     I(n_tokens_title^2) +
                     I(num_videos^2) +
                     I(average_token_length^2) + 
                     I(kw_min_avg^2) +
                     I(kw_avg_avg^2) +
                     I(self_reference_avg_sharess^2) +
                     I(title_sentiment_polarity^2) +
                     num_hrefs:num_imgs +
                     kw_min_min:weekday_is_tuesday +
                     n_tokens_title:num_imgs,
            data = world_news_train)

summary(lm1Fit)
```

## Linear Model #2

We built this model using our best judgment given what we knew about the data. Using all 52 of the predictors (and their interactions) would not be manageable or parsimonious. We chose a limited number of predictors using EDA and common sense in order to limit colinearity and maintain simplicity of the model.

First, we selected variables that we believed would have be the "best" predictors of the number of shares. Recall that many of the predictors could be grouped into categories based on the common attributes each measured. We made an effort to pick at least one predictor from each category. For example, of the 8 variables that measure day of publication, we chose only `is_weekend` to include in our linear model; or, of the 10 variables that measure keyword attributes, we chose only `num_keywords` to include in our linear model. At the end of this step, we landed on using 13 predictors. We decided not to include higher order predictors to reduce complexity.

Next, we fit a full model using all 13 of our selected predictors with all their associated interactions. This produced a very high dimensional model. To quickly obtain parsimony, we "unscientifically" selected to include into a reduced model our 13 predictors and only the interaction terms that were highly significant within the full model. There were only 7 of these interaction terms. This reduced model is what is used in the code below for building Linear Model #2.

```{r John Linear Model}
# Remove variables with possible colinearity
reduced_world_news_data <- world_news_data %>%
                           select(n_tokens_title,
                                  n_tokens_content,
                                  num_hrefs,
                                  num_imgs,
                                  num_videos,
                                  num_keywords,
                                  average_token_length,
                                  is_weekend,
                                  global_subjectivity,
                                  global_sentiment_polarity,
                                  title_sentiment_polarity,
                                  self_reference_avg_sharess,
                                  rate_positive_words,
                                  shares)

lm2Fit <- lm(log(shares) ~ . + n_tokens_title:average_token_length +
                               n_tokens_title:global_sentiment_polarity +
                               n_tokens_title:rate_positive_words +
                               n_tokens_content:num_imgs +
                               num_imgs:num_keywords +
                               num_imgs:global_subjectivity +
                               num_imgs:global_sentiment_polarity, 
             data = reduced_world_news_data)

summary(lm2Fit)
```

## Random Forest Model

The random forest model is a classification algorithm that consists of many decision trees. It creates multiple (e.g. 500) bootstrapped samples and fit decision trees to each of the bootstrapped samples. When selecting a split point, the learning algorithm selects a random sample of predictors of which to search, instead of all the predictors. By not looking at all the predictors every time, it prevents one or two strong predictors to dominate the tree fits. The prediction of trees are then averaged (for regression) to get the final predicted value. This would lead to less variance and better fit over an individual tree fit.

Regarding the number of predictors to look for, a good rule of thumb is m = sqrt(p) for classification and m=p/3 for regression, where m is the number of randomly selected features at each point, and p is the number of input variables. In our dataset there are 52 predictors so we will be using m = 17.  

```{r Joey random Forest, cache=TRUE}
# Fit the random forest Model
rfFit <- train(log(shares) ~ .,
               data = world_news_train,
               method = "rf",
               trControl = trainControl(method = "cv", number = 5),
               preProcess = c("center", "scale"),
               tuneGrid = data.frame(mtry = 1:17))
```


## Boosted Tree Model

Just like the random forest model, the boosted tree model (sometimes called gradient boosted tree) combines outputs from individual decision trees to create a prediction model. It differs from the random forest model by combining weak learners sequentially so that each new tree corrects errors of the previous one. In this case, weak learners are decision trees that perform relatively poorly, not much better than random guessing. Sequentially combining these poor performing models to create a better fitting model is called boosting. It's analogous to the proverb "None of us is as smart as all of us."

The first step in creating a boosted tree model is fitting a single decision tree with $d$ splits to the data. We evaluate this fit using a loss function, a method of measuring prediction error. There are many different loss functions and its selection is arbitrary. Step 2 is to add a second decision tree (also with $d$ splits) to the first such that it lowers the loss compared to the first tree alone:

$$Boosted Ensemble = First Tree + \lambda * Second Tree$$

$$Loss(Boosted Ensemble) < Loss(First Tree)$$

Here, $\lambda$ is a shrinkage parameter which slows the fitting process. It's sometimes called the learning rate. We repeat the second step $B$ times to finish building the model. The tuning parameters $\lambda$, $d$, and $B$ can be chosen using cross validation. 

In the R code below which uses the `caret` package to build a boosted tree model, `n.trees` is $B$, `interaction.depth` is $d$, and `shrinkage` is $\lambda$. The additional tuning parameter `n.minobsinnode` allows for controlling the minimum number of observations within each tree node.

```{r John Boosted Tree, cache=TRUE}
control <- trainControl(method = "cv", number = 5)
boostedFit <- train(log(shares) ~ ., data = world_news_train, 
                    method = "gbm",
                    trControl = control,
                    preProcess = c("center", "scale"),
                    tuneGrid = expand.grid(n.trees = c(300, 350, 400, 450, 500),
                                           interaction.depth = c(14:24),
                                           shrinkage = c(0.01),
                                           n.minobsinnode = 10),
                    verbose = FALSE)
```

# Model Evaluation

We can compare the RMSE, Rsquared, and MAE of the four model fits on the test data. We will be choosing the model with the lowest RMSE.

```{r compare training}
prediction <- predict(lm1Fit, newdata = world_news_test)
lm1 <- round(postResample(prediction, log(world_news_test$shares)), 4)

prediction <- predict(lm2Fit, newdata = world_news_test)
lm2 <- round(postResample(prediction, log(world_news_test$shares)), 4)

prediction <- predict(rfFit, newdata = world_news_test)
rf <- round(postResample(prediction, log(world_news_test$shares)), 4)

prediction <- predict(boostedFit, newdata = world_news_test)
boost <- round(postResample(prediction, log(world_news_test$shares)), 4)

compareFits <- data.frame(lm1, lm2, rf, boost)
names(compareFits) <- c("Linear Model 1", 
                        "Linear Model 2",
                        "Random Forest Model",
                        "Boosted Tree Model")

compareFitsLong <- data.frame(t(compareFits))

# Sort the table of models by ascending RMSE and pick the first row as the model
finalModel <- compareFitsLong %>% arrange(RMSE) %>% filter(row_number()==1) 

knitr::kable(compareFitsLong)
```
  
Our final model is the `r row.names(finalModel)`. When we fit the model on the test data we get RMSE=`r finalModel[1, 1]`,  Rsquared=`r finalModel[1, 2]`, and MAE=`r finalModel[1, 3]`.
